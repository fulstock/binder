{
    "epoch": 46.46,
    "eval_AGE-f1": 0.9142857142857143,
    "eval_AGE-fn": 9.0,
    "eval_AGE-fp": 15.0,
    "eval_AGE-precision": 0.8951048951048951,
    "eval_AGE-recall": 0.9343065693430657,
    "eval_AGE-tp": 128.0,
    "eval_AWARD-f1": 0.6881720430107527,
    "eval_AWARD-fn": 9.0,
    "eval_AWARD-fp": 20.0,
    "eval_AWARD-precision": 0.6153846153846154,
    "eval_AWARD-recall": 0.7804878048780488,
    "eval_AWARD-tp": 32.0,
    "eval_CITY-f1": 0.9227166276346604,
    "eval_CITY-fn": 11.0,
    "eval_CITY-fp": 22.0,
    "eval_CITY-precision": 0.8995433789954338,
    "eval_CITY-recall": 0.9471153846153846,
    "eval_CITY-tp": 197.0,
    "eval_COUNTRY-f1": 0.9439775910364145,
    "eval_COUNTRY-fn": 18.0,
    "eval_COUNTRY-fp": 22.0,
    "eval_COUNTRY-precision": 0.9387186629526463,
    "eval_COUNTRY-recall": 0.9492957746478873,
    "eval_COUNTRY-tp": 337.0,
    "eval_CRIME-f1": 0.496,
    "eval_CRIME-fn": 24.0,
    "eval_CRIME-fp": 39.0,
    "eval_CRIME-precision": 0.44285714285714284,
    "eval_CRIME-recall": 0.5636363636363636,
    "eval_CRIME-tp": 31.0,
    "eval_DATE-f1": 0.8935762224352828,
    "eval_DATE-fn": 58.0,
    "eval_DATE-fp": 53.0,
    "eval_DATE-precision": 0.8978805394990366,
    "eval_DATE-recall": 0.8893129770992366,
    "eval_DATE-tp": 466.0,
    "eval_DISEASE-f1": 0.5897435897435898,
    "eval_DISEASE-fn": 43.0,
    "eval_DISEASE-fp": 53.0,
    "eval_DISEASE-precision": 0.5655737704918032,
    "eval_DISEASE-recall": 0.6160714285714286,
    "eval_DISEASE-tp": 69.0,
    "eval_DISTRICT-f1": 0.5641025641025641,
    "eval_DISTRICT-fn": 6.0,
    "eval_DISTRICT-fp": 11.0,
    "eval_DISTRICT-precision": 0.5,
    "eval_DISTRICT-recall": 0.6470588235294118,
    "eval_DISTRICT-tp": 11.0,
    "eval_EVENT-f1": 0.653250773993808,
    "eval_EVENT-fn": 259.0,
    "eval_EVENT-fp": 189.0,
    "eval_EVENT-precision": 0.690671031096563,
    "eval_EVENT-recall": 0.6196769456681351,
    "eval_EVENT-tp": 422.0,
    "eval_FACILITY-f1": 0.5868263473053892,
    "eval_FACILITY-fn": 35.0,
    "eval_FACILITY-fp": 34.0,
    "eval_FACILITY-precision": 0.5903614457831325,
    "eval_FACILITY-recall": 0.5833333333333334,
    "eval_FACILITY-tp": 49.0,
    "eval_FAMILY-f1": 0.6153846153846154,
    "eval_FAMILY-fn": 2.0,
    "eval_FAMILY-fp": 3.0,
    "eval_FAMILY-precision": 0.5714285714285714,
    "eval_FAMILY-recall": 0.6666666666666666,
    "eval_FAMILY-tp": 4.0,
    "eval_IDEOLOGY-f1": 0.7605633802816901,
    "eval_IDEOLOGY-fn": 9.0,
    "eval_IDEOLOGY-fp": 8.0,
    "eval_IDEOLOGY-precision": 0.7714285714285715,
    "eval_IDEOLOGY-recall": 0.75,
    "eval_IDEOLOGY-tp": 27.0,
    "eval_LANGUAGE-f1": 0.5,
    "eval_LANGUAGE-fn": 4.0,
    "eval_LANGUAGE-fp": 2.0,
    "eval_LANGUAGE-precision": 0.6,
    "eval_LANGUAGE-recall": 0.42857142857142855,
    "eval_LANGUAGE-tp": 3.0,
    "eval_LAW-f1": 0.6845637583892618,
    "eval_LAW-fn": 32.0,
    "eval_LAW-fp": 15.0,
    "eval_LAW-precision": 0.7727272727272727,
    "eval_LAW-recall": 0.6144578313253012,
    "eval_LAW-tp": 51.0,
    "eval_LOCATION-f1": 0.7777777777777778,
    "eval_LOCATION-fn": 15.0,
    "eval_LOCATION-fp": 13.0,
    "eval_LOCATION-precision": 0.7903225806451613,
    "eval_LOCATION-recall": 0.765625,
    "eval_LOCATION-tp": 49.0,
    "eval_MONEY-f1": 0.847457627118644,
    "eval_MONEY-fn": 4.0,
    "eval_MONEY-fp": 5.0,
    "eval_MONEY-precision": 0.8333333333333334,
    "eval_MONEY-recall": 0.8620689655172413,
    "eval_MONEY-tp": 25.0,
    "eval_NATIONALITY-f1": 0.8245614035087719,
    "eval_NATIONALITY-fn": 11.0,
    "eval_NATIONALITY-fp": 9.0,
    "eval_NATIONALITY-precision": 0.8392857142857143,
    "eval_NATIONALITY-recall": 0.8103448275862069,
    "eval_NATIONALITY-tp": 47.0,
    "eval_NUMBER-f1": 0.8977272727272727,
    "eval_NUMBER-fn": 26.0,
    "eval_NUMBER-fp": 10.0,
    "eval_NUMBER-precision": 0.9404761904761905,
    "eval_NUMBER-recall": 0.8586956521739131,
    "eval_NUMBER-tp": 158.0,
    "eval_ORDINAL-f1": 0.85,
    "eval_ORDINAL-fn": 16.0,
    "eval_ORDINAL-fp": 14.0,
    "eval_ORDINAL-precision": 0.8585858585858586,
    "eval_ORDINAL-recall": 0.8415841584158416,
    "eval_ORDINAL-tp": 85.0,
    "eval_ORGANIZATION-f1": 0.8629600626468285,
    "eval_ORGANIZATION-fn": 65.0,
    "eval_ORGANIZATION-fp": 110.0,
    "eval_ORGANIZATION-precision": 0.8335854765506808,
    "eval_ORGANIZATION-recall": 0.8944805194805194,
    "eval_ORGANIZATION-tp": 551.0,
    "eval_PENALTY-f1": 0.6923076923076923,
    "eval_PENALTY-fn": 21.0,
    "eval_PENALTY-fp": 11.0,
    "eval_PENALTY-precision": 0.7659574468085106,
    "eval_PENALTY-recall": 0.631578947368421,
    "eval_PENALTY-tp": 36.0,
    "eval_PERCENT-f1": 0.9,
    "eval_PERCENT-fn": 0.0,
    "eval_PERCENT-fp": 2.0,
    "eval_PERCENT-precision": 0.8181818181818182,
    "eval_PERCENT-recall": 1.0,
    "eval_PERCENT-tp": 9.0,
    "eval_PERSON-f1": 0.9688325409403064,
    "eval_PERSON-fn": 32.0,
    "eval_PERSON-fp": 27.0,
    "eval_PERSON-precision": 0.9713983050847458,
    "eval_PERSON-recall": 0.9662802950474183,
    "eval_PERSON-tp": 917.0,
    "eval_PRODUCT-f1": 0.6582278481012658,
    "eval_PRODUCT-fn": 4.0,
    "eval_PRODUCT-fp": 23.0,
    "eval_PRODUCT-precision": 0.5306122448979592,
    "eval_PRODUCT-recall": 0.8666666666666667,
    "eval_PRODUCT-tp": 26.0,
    "eval_PROFESSION-f1": 0.8619883040935673,
    "eval_PROFESSION-fn": 116.0,
    "eval_PROFESSION-fp": 120.0,
    "eval_PROFESSION-precision": 0.8599766627771295,
    "eval_PROFESSION-recall": 0.8640093786635404,
    "eval_PROFESSION-tp": 737.0,
    "eval_RELIGION-f1": 0.8,
    "eval_RELIGION-fn": 3.0,
    "eval_RELIGION-fp": 0.0,
    "eval_RELIGION-precision": 1.0,
    "eval_RELIGION-recall": 0.6666666666666666,
    "eval_RELIGION-tp": 6.0,
    "eval_STATE_OR_PROVINCE-f1": 0.8787878787878788,
    "eval_STATE_OR_PROVINCE-fn": 12.0,
    "eval_STATE_OR_PROVINCE-fp": 12.0,
    "eval_STATE_OR_PROVINCE-precision": 0.8787878787878788,
    "eval_STATE_OR_PROVINCE-recall": 0.8787878787878788,
    "eval_STATE_OR_PROVINCE-tp": 87.0,
    "eval_TIME-f1": 0.847457627118644,
    "eval_TIME-fn": 4.0,
    "eval_TIME-fp": 5.0,
    "eval_TIME-precision": 0.8333333333333334,
    "eval_TIME-recall": 0.8620689655172413,
    "eval_TIME-tp": 25.0,
    "eval_WORK_OF_ART-f1": 0.8416289592760181,
    "eval_WORK_OF_ART-fn": 11.0,
    "eval_WORK_OF_ART-fp": 24.0,
    "eval_WORK_OF_ART-precision": 0.7948717948717948,
    "eval_WORK_OF_ART-recall": 0.8942307692307693,
    "eval_WORK_OF_ART-tp": 93.0,
    "eval_f1": 0.769754421448566,
    "eval_macro-f1": 0.769754421448566,
    "eval_macro-precision": 0.7689789150472343,
    "eval_macro-recall": 0.7811406904485524,
    "eval_micro-f1": 0.8439473209453364,
    "eval_micro-precision": 0.8430347810416291,
    "eval_micro-recall": 0.844861838540726,
    "eval_precision": 0.7689789150472343,
    "eval_recall": 0.7811406904485524,
    "eval_samples": 190,
    "predict_samples": 205,
    "test_AGE-f1": 0.9084249084249084,
    "test_AGE-fn": 14.0,
    "test_AGE-fp": 11.0,
    "test_AGE-precision": 0.9185185185185185,
    "test_AGE-recall": 0.8985507246376812,
    "test_AGE-tp": 124.0,
    "test_AWARD-f1": 0.746268656716418,
    "test_AWARD-fn": 19.0,
    "test_AWARD-fp": 49.0,
    "test_AWARD-precision": 0.6711409395973155,
    "test_AWARD-recall": 0.8403361344537815,
    "test_AWARD-tp": 100.0,
    "test_CITY-f1": 0.9452332657200812,
    "test_CITY-fn": 7.0,
    "test_CITY-fp": 20.0,
    "test_CITY-precision": 0.9209486166007905,
    "test_CITY-recall": 0.9708333333333333,
    "test_CITY-tp": 233.0,
    "test_COUNTRY-f1": 0.95361380798274,
    "test_COUNTRY-fn": 16.0,
    "test_COUNTRY-fp": 27.0,
    "test_COUNTRY-precision": 0.9424307036247335,
    "test_COUNTRY-recall": 0.9650655021834061,
    "test_COUNTRY-tp": 442.0,
    "test_CRIME-f1": 0.6436781609195402,
    "test_CRIME-fn": 7.0,
    "test_CRIME-fp": 24.0,
    "test_CRIME-precision": 0.5384615384615384,
    "test_CRIME-recall": 0.8,
    "test_CRIME-tp": 28.0,
    "test_DATE-f1": 0.8920454545454546,
    "test_DATE-fn": 58.0,
    "test_DATE-fp": 56.0,
    "test_DATE-precision": 0.8937381404174574,
    "test_DATE-recall": 0.8903591682419659,
    "test_DATE-tp": 471.0,
    "test_DISEASE-f1": 0.7368421052631579,
    "test_DISEASE-fn": 14.0,
    "test_DISEASE-fp": 16.0,
    "test_DISEASE-precision": 0.7241379310344828,
    "test_DISEASE-recall": 0.75,
    "test_DISEASE-tp": 42.0,
    "test_DISTRICT-f1": 0.7547169811320755,
    "test_DISTRICT-fn": 5.0,
    "test_DISTRICT-fp": 8.0,
    "test_DISTRICT-precision": 0.7142857142857143,
    "test_DISTRICT-recall": 0.8,
    "test_DISTRICT-tp": 20.0,
    "test_EVENT-f1": 0.6626323751891074,
    "test_EVENT-fn": 214.0,
    "test_EVENT-fp": 232.0,
    "test_EVENT-precision": 0.6537313432835821,
    "test_EVENT-recall": 0.6717791411042945,
    "test_EVENT-tp": 438.0,
    "test_FACILITY-f1": 0.75,
    "test_FACILITY-fn": 15.0,
    "test_FACILITY-fp": 17.0,
    "test_FACILITY-precision": 0.7384615384615385,
    "test_FACILITY-recall": 0.7619047619047619,
    "test_FACILITY-tp": 48.0,
    "test_FAMILY-f1": 0.3157894736842105,
    "test_FAMILY-fn": 11.0,
    "test_FAMILY-fp": 2.0,
    "test_FAMILY-precision": 0.6,
    "test_FAMILY-recall": 0.21428571428571427,
    "test_FAMILY-tp": 3.0,
    "test_IDEOLOGY-f1": 0.6753246753246753,
    "test_IDEOLOGY-fn": 18.0,
    "test_IDEOLOGY-fp": 7.0,
    "test_IDEOLOGY-precision": 0.7878787878787878,
    "test_IDEOLOGY-recall": 0.5909090909090909,
    "test_IDEOLOGY-tp": 26.0,
    "test_LANGUAGE-f1": 0.7368421052631579,
    "test_LANGUAGE-fn": 3.0,
    "test_LANGUAGE-fp": 2.0,
    "test_LANGUAGE-precision": 0.7777777777777778,
    "test_LANGUAGE-recall": 0.7,
    "test_LANGUAGE-tp": 7.0,
    "test_LAW-f1": 0.7241379310344828,
    "test_LAW-fn": 19.0,
    "test_LAW-fp": 13.0,
    "test_LAW-precision": 0.7636363636363637,
    "test_LAW-recall": 0.6885245901639344,
    "test_LAW-tp": 42.0,
    "test_LOCATION-f1": 0.6785714285714286,
    "test_LOCATION-fn": 23.0,
    "test_LOCATION-fp": 13.0,
    "test_LOCATION-precision": 0.7450980392156863,
    "test_LOCATION-recall": 0.6229508196721312,
    "test_LOCATION-tp": 38.0,
    "test_MONEY-f1": 0.717391304347826,
    "test_MONEY-fn": 10.0,
    "test_MONEY-fp": 16.0,
    "test_MONEY-precision": 0.673469387755102,
    "test_MONEY-recall": 0.7674418604651163,
    "test_MONEY-tp": 33.0,
    "test_NATIONALITY-f1": 0.8062015503875969,
    "test_NATIONALITY-fn": 11.0,
    "test_NATIONALITY-fp": 14.0,
    "test_NATIONALITY-precision": 0.7878787878787878,
    "test_NATIONALITY-recall": 0.8253968253968254,
    "test_NATIONALITY-tp": 52.0,
    "test_NUMBER-f1": 0.8287037037037037,
    "test_NUMBER-fn": 50.0,
    "test_NUMBER-fp": 24.0,
    "test_NUMBER-precision": 0.8817733990147784,
    "test_NUMBER-recall": 0.7816593886462883,
    "test_NUMBER-tp": 179.0,
    "test_ORDINAL-f1": 0.8708133971291866,
    "test_ORDINAL-fn": 15.0,
    "test_ORDINAL-fp": 12.0,
    "test_ORDINAL-precision": 0.883495145631068,
    "test_ORDINAL-recall": 0.8584905660377359,
    "test_ORDINAL-tp": 91.0,
    "test_ORGANIZATION-f1": 0.8444767441860465,
    "test_ORGANIZATION-fn": 92.0,
    "test_ORGANIZATION-fp": 122.0,
    "test_ORGANIZATION-precision": 0.8264580369843528,
    "test_ORGANIZATION-recall": 0.8632986627043091,
    "test_ORGANIZATION-tp": 581.0,
    "test_PENALTY-f1": 0.6153846153846154,
    "test_PENALTY-fn": 5.0,
    "test_PENALTY-fp": 10.0,
    "test_PENALTY-precision": 0.5454545454545454,
    "test_PENALTY-recall": 0.7058823529411765,
    "test_PENALTY-tp": 12.0,
    "test_PERCENT-f1": 0.6153846153846154,
    "test_PERCENT-fn": 3.0,
    "test_PERCENT-fp": 2.0,
    "test_PERCENT-precision": 0.6666666666666666,
    "test_PERCENT-recall": 0.5714285714285714,
    "test_PERCENT-tp": 4.0,
    "test_PERSON-f1": 0.9767921609076844,
    "test_PERSON-fn": 18.0,
    "test_PERSON-fp": 27.0,
    "test_PERSON-precision": 0.9722792607802875,
    "test_PERSON-recall": 0.9813471502590674,
    "test_PERSON-tp": 947.0,
    "test_PRODUCT-f1": 0.7647058823529411,
    "test_PRODUCT-fn": 14.0,
    "test_PRODUCT-fp": 10.0,
    "test_PRODUCT-precision": 0.7959183673469388,
    "test_PRODUCT-recall": 0.7358490566037735,
    "test_PRODUCT-tp": 39.0,
    "test_PROFESSION-f1": 0.8409356725146199,
    "test_PROFESSION-fn": 130.0,
    "test_PROFESSION-fp": 142.0,
    "test_PROFESSION-precision": 0.835075493612079,
    "test_PROFESSION-recall": 0.8468786808009423,
    "test_PROFESSION-tp": 719.0,
    "test_RELIGION-f1": 0.8181818181818182,
    "test_RELIGION-fn": 5.0,
    "test_RELIGION-fp": 3.0,
    "test_RELIGION-precision": 0.8571428571428571,
    "test_RELIGION-recall": 0.782608695652174,
    "test_RELIGION-tp": 18.0,
    "test_STATE_OR_PROVINCE-f1": 0.9066666666666666,
    "test_STATE_OR_PROVINCE-fn": 10.0,
    "test_STATE_OR_PROVINCE-fp": 11.0,
    "test_STATE_OR_PROVINCE-precision": 0.9026548672566371,
    "test_STATE_OR_PROVINCE-recall": 0.9107142857142857,
    "test_STATE_OR_PROVINCE-tp": 102.0,
    "test_TIME-f1": 0.8043478260869565,
    "test_TIME-fn": 10.0,
    "test_TIME-fp": 8.0,
    "test_TIME-precision": 0.8222222222222222,
    "test_TIME-recall": 0.7872340425531915,
    "test_TIME-tp": 37.0,
    "test_WORK_OF_ART-f1": 0.8044692737430168,
    "test_WORK_OF_ART-fn": 22.0,
    "test_WORK_OF_ART-fp": 13.0,
    "test_WORK_OF_ART-precision": 0.8470588235294118,
    "test_WORK_OF_ART-recall": 0.7659574468085106,
    "test_WORK_OF_ART-tp": 72.0,
    "test_f1": 0.7702957434740942,
    "test_macro-f1": 0.7702957434740942,
    "test_macro-precision": 0.7823377177265527,
    "test_macro-recall": 0.770678847134554,
    "test_micro-f1": 0.8498067840274796,
    "test_micro-precision": 0.8445127154804574,
    "test_micro-recall": 0.8551676460421708,
    "test_precision": 0.7823377177265527,
    "test_recall": 0.770678847134554,
    "train_loss": 0.17724036476534347,
    "train_runtime": 4983.8518,
    "train_samples": 1576,
    "train_samples_per_second": 20.238,
    "train_steps_per_second": 1.271
}