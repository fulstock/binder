{
    "epoch": 46.46,
    "predict_samples": 205,
    "test_AGE-f1": 0.9084249084249084,
    "test_AGE-fn": 14.0,
    "test_AGE-fp": 11.0,
    "test_AGE-precision": 0.9185185185185185,
    "test_AGE-recall": 0.8985507246376812,
    "test_AGE-tp": 124.0,
    "test_AWARD-f1": 0.746268656716418,
    "test_AWARD-fn": 19.0,
    "test_AWARD-fp": 49.0,
    "test_AWARD-precision": 0.6711409395973155,
    "test_AWARD-recall": 0.8403361344537815,
    "test_AWARD-tp": 100.0,
    "test_CITY-f1": 0.9452332657200812,
    "test_CITY-fn": 7.0,
    "test_CITY-fp": 20.0,
    "test_CITY-precision": 0.9209486166007905,
    "test_CITY-recall": 0.9708333333333333,
    "test_CITY-tp": 233.0,
    "test_COUNTRY-f1": 0.95361380798274,
    "test_COUNTRY-fn": 16.0,
    "test_COUNTRY-fp": 27.0,
    "test_COUNTRY-precision": 0.9424307036247335,
    "test_COUNTRY-recall": 0.9650655021834061,
    "test_COUNTRY-tp": 442.0,
    "test_CRIME-f1": 0.6436781609195402,
    "test_CRIME-fn": 7.0,
    "test_CRIME-fp": 24.0,
    "test_CRIME-precision": 0.5384615384615384,
    "test_CRIME-recall": 0.8,
    "test_CRIME-tp": 28.0,
    "test_DATE-f1": 0.8920454545454546,
    "test_DATE-fn": 58.0,
    "test_DATE-fp": 56.0,
    "test_DATE-precision": 0.8937381404174574,
    "test_DATE-recall": 0.8903591682419659,
    "test_DATE-tp": 471.0,
    "test_DISEASE-f1": 0.7368421052631579,
    "test_DISEASE-fn": 14.0,
    "test_DISEASE-fp": 16.0,
    "test_DISEASE-precision": 0.7241379310344828,
    "test_DISEASE-recall": 0.75,
    "test_DISEASE-tp": 42.0,
    "test_DISTRICT-f1": 0.7547169811320755,
    "test_DISTRICT-fn": 5.0,
    "test_DISTRICT-fp": 8.0,
    "test_DISTRICT-precision": 0.7142857142857143,
    "test_DISTRICT-recall": 0.8,
    "test_DISTRICT-tp": 20.0,
    "test_EVENT-f1": 0.6626323751891074,
    "test_EVENT-fn": 214.0,
    "test_EVENT-fp": 232.0,
    "test_EVENT-precision": 0.6537313432835821,
    "test_EVENT-recall": 0.6717791411042945,
    "test_EVENT-tp": 438.0,
    "test_FACILITY-f1": 0.75,
    "test_FACILITY-fn": 15.0,
    "test_FACILITY-fp": 17.0,
    "test_FACILITY-precision": 0.7384615384615385,
    "test_FACILITY-recall": 0.7619047619047619,
    "test_FACILITY-tp": 48.0,
    "test_FAMILY-f1": 0.3157894736842105,
    "test_FAMILY-fn": 11.0,
    "test_FAMILY-fp": 2.0,
    "test_FAMILY-precision": 0.6,
    "test_FAMILY-recall": 0.21428571428571427,
    "test_FAMILY-tp": 3.0,
    "test_IDEOLOGY-f1": 0.6753246753246753,
    "test_IDEOLOGY-fn": 18.0,
    "test_IDEOLOGY-fp": 7.0,
    "test_IDEOLOGY-precision": 0.7878787878787878,
    "test_IDEOLOGY-recall": 0.5909090909090909,
    "test_IDEOLOGY-tp": 26.0,
    "test_LANGUAGE-f1": 0.7368421052631579,
    "test_LANGUAGE-fn": 3.0,
    "test_LANGUAGE-fp": 2.0,
    "test_LANGUAGE-precision": 0.7777777777777778,
    "test_LANGUAGE-recall": 0.7,
    "test_LANGUAGE-tp": 7.0,
    "test_LAW-f1": 0.7241379310344828,
    "test_LAW-fn": 19.0,
    "test_LAW-fp": 13.0,
    "test_LAW-precision": 0.7636363636363637,
    "test_LAW-recall": 0.6885245901639344,
    "test_LAW-tp": 42.0,
    "test_LOCATION-f1": 0.6785714285714286,
    "test_LOCATION-fn": 23.0,
    "test_LOCATION-fp": 13.0,
    "test_LOCATION-precision": 0.7450980392156863,
    "test_LOCATION-recall": 0.6229508196721312,
    "test_LOCATION-tp": 38.0,
    "test_MONEY-f1": 0.717391304347826,
    "test_MONEY-fn": 10.0,
    "test_MONEY-fp": 16.0,
    "test_MONEY-precision": 0.673469387755102,
    "test_MONEY-recall": 0.7674418604651163,
    "test_MONEY-tp": 33.0,
    "test_NATIONALITY-f1": 0.8062015503875969,
    "test_NATIONALITY-fn": 11.0,
    "test_NATIONALITY-fp": 14.0,
    "test_NATIONALITY-precision": 0.7878787878787878,
    "test_NATIONALITY-recall": 0.8253968253968254,
    "test_NATIONALITY-tp": 52.0,
    "test_NUMBER-f1": 0.8287037037037037,
    "test_NUMBER-fn": 50.0,
    "test_NUMBER-fp": 24.0,
    "test_NUMBER-precision": 0.8817733990147784,
    "test_NUMBER-recall": 0.7816593886462883,
    "test_NUMBER-tp": 179.0,
    "test_ORDINAL-f1": 0.8708133971291866,
    "test_ORDINAL-fn": 15.0,
    "test_ORDINAL-fp": 12.0,
    "test_ORDINAL-precision": 0.883495145631068,
    "test_ORDINAL-recall": 0.8584905660377359,
    "test_ORDINAL-tp": 91.0,
    "test_ORGANIZATION-f1": 0.8444767441860465,
    "test_ORGANIZATION-fn": 92.0,
    "test_ORGANIZATION-fp": 122.0,
    "test_ORGANIZATION-precision": 0.8264580369843528,
    "test_ORGANIZATION-recall": 0.8632986627043091,
    "test_ORGANIZATION-tp": 581.0,
    "test_PENALTY-f1": 0.6153846153846154,
    "test_PENALTY-fn": 5.0,
    "test_PENALTY-fp": 10.0,
    "test_PENALTY-precision": 0.5454545454545454,
    "test_PENALTY-recall": 0.7058823529411765,
    "test_PENALTY-tp": 12.0,
    "test_PERCENT-f1": 0.6153846153846154,
    "test_PERCENT-fn": 3.0,
    "test_PERCENT-fp": 2.0,
    "test_PERCENT-precision": 0.6666666666666666,
    "test_PERCENT-recall": 0.5714285714285714,
    "test_PERCENT-tp": 4.0,
    "test_PERSON-f1": 0.9767921609076844,
    "test_PERSON-fn": 18.0,
    "test_PERSON-fp": 27.0,
    "test_PERSON-precision": 0.9722792607802875,
    "test_PERSON-recall": 0.9813471502590674,
    "test_PERSON-tp": 947.0,
    "test_PRODUCT-f1": 0.7647058823529411,
    "test_PRODUCT-fn": 14.0,
    "test_PRODUCT-fp": 10.0,
    "test_PRODUCT-precision": 0.7959183673469388,
    "test_PRODUCT-recall": 0.7358490566037735,
    "test_PRODUCT-tp": 39.0,
    "test_PROFESSION-f1": 0.8409356725146199,
    "test_PROFESSION-fn": 130.0,
    "test_PROFESSION-fp": 142.0,
    "test_PROFESSION-precision": 0.835075493612079,
    "test_PROFESSION-recall": 0.8468786808009423,
    "test_PROFESSION-tp": 719.0,
    "test_RELIGION-f1": 0.8181818181818182,
    "test_RELIGION-fn": 5.0,
    "test_RELIGION-fp": 3.0,
    "test_RELIGION-precision": 0.8571428571428571,
    "test_RELIGION-recall": 0.782608695652174,
    "test_RELIGION-tp": 18.0,
    "test_STATE_OR_PROVINCE-f1": 0.9066666666666666,
    "test_STATE_OR_PROVINCE-fn": 10.0,
    "test_STATE_OR_PROVINCE-fp": 11.0,
    "test_STATE_OR_PROVINCE-precision": 0.9026548672566371,
    "test_STATE_OR_PROVINCE-recall": 0.9107142857142857,
    "test_STATE_OR_PROVINCE-tp": 102.0,
    "test_TIME-f1": 0.8043478260869565,
    "test_TIME-fn": 10.0,
    "test_TIME-fp": 8.0,
    "test_TIME-precision": 0.8222222222222222,
    "test_TIME-recall": 0.7872340425531915,
    "test_TIME-tp": 37.0,
    "test_WORK_OF_ART-f1": 0.8044692737430168,
    "test_WORK_OF_ART-fn": 22.0,
    "test_WORK_OF_ART-fp": 13.0,
    "test_WORK_OF_ART-precision": 0.8470588235294118,
    "test_WORK_OF_ART-recall": 0.7659574468085106,
    "test_WORK_OF_ART-tp": 72.0,
    "test_f1": 0.7702957434740942,
    "test_macro-f1": 0.7702957434740942,
    "test_macro-precision": 0.7823377177265527,
    "test_macro-recall": 0.770678847134554,
    "test_micro-f1": 0.8498067840274796,
    "test_micro-precision": 0.8445127154804574,
    "test_micro-recall": 0.8551676460421708,
    "test_precision": 0.7823377177265527,
    "test_recall": 0.770678847134554
}